{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e50aa8f9",
   "metadata": {},
   "source": [
    "# Learning Project: Document Classification with PyTorch and TorchText\n",
    "\n",
    "This project builds a text classification model using the AG_NEWS dataset with PyTorch and TorchText. The model predicts the category of a news article (World, Sports, Business, or Sci/Tech) from its raw text. It uses:\n",
    "- __Tokenization__ and __vocabulary building__ to convert raw text into numeric format\n",
    "- A __collate function__ with EmbeddingBag for efficient text representation without padding\n",
    "- A __simple feedforward neural network__ for classification\n",
    "- __Cross-entropy loss__ and __stochastic gradient descent (SGD)__ for training\n",
    "- A learning __rate scheduler__ to dynamically adjust training speed\n",
    "\n",
    "The project demonstrates the full NLP pipeline: from data preprocessing to model training, evaluation, and optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0790b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries \n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b5d9278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split \n",
    "train_iter, test_iter = AG_NEWS(split=('train','test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c376b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizaiton and Vocab building \n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "# build vocabulary (with generator function to avoid memory inefficiency)\n",
    "#def yeild_tokens(data_iter): \n",
    "#    for _,text in data_iter: \n",
    "#        yield tokenizer(text)\n",
    "vocab = build_vocab_from_iterator(\n",
    "    (tokenizer(text) for _,text in train_iter), \n",
    "    specials=['<unk>']\n",
    ")\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c5b4e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate function \n",
    "## collate function essentially preprocess the batch on the fly \n",
    "def collate_batch(batch): \n",
    "    # label_list -> store the true class label as int\n",
    "    # text_list -> store tokenized & indexed text \n",
    "    # offset -> starting index of each sample \n",
    "    label_list, text_list, offset = [],[],[0]\n",
    "    for label,text in batch: \n",
    "        label_list.append(label - 1) # -1 for 0 indexed \n",
    "        processed_text = torch.tensor(vocab(tokenizer(text)), dtype=torch.int64) # tokenized & numericalized\n",
    "        text_list.append(processed_text)\n",
    "        offset.append(processed_text.size(0)) # basically length of tensor .size is a tensor method to get dim\n",
    "    # convert label list to label tensor \n",
    "    label_tensor = torch.tensor(label_list,dtype=torch.int64)\n",
    "    # convert text list to text tensor \n",
    "    text_tensor = torch.cat(text_list)\n",
    "    # cumulative offsets: start index of each sample \n",
    "    offset_tensor = torch.tensor(offset[:-1]).cumsum(dim=0) # last length no needed; dim = 0 row wised \n",
    "\n",
    "    return label_tensor,text_tensor,offset_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbd30200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader \n",
    "from torch.utils.data.dataset import random_split\n",
    "# 95% for training and 5% for validation \n",
    "t_len = len(list(AG_NEWS(split='train')))\n",
    "num_train = int(t_len * 0.95)\n",
    "num_valid = t_len - num_train\n",
    "# randomly split training set into training and validation \n",
    "train_set, valid_set = random_split(list(AG_NEWS(split='train')), [num_train,num_valid])\n",
    "# wrap training set into train_dataloader \n",
    "train_dataloader = DataLoader(train_set,batch_size=8,shuffle=True,collate_fn=collate_batch)\n",
    "# wrap validation set into valid_dataloader \n",
    "valid_dataloader = DataLoader(valid_set,batch_size=8,collate_fn=collate_batch) # no shuffle, deterministic \n",
    "# wrap test set into test_dataloader \n",
    "test_dataloader = DataLoader(list(AG_NEWS(split='test')), batch_size=8,collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca159ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model \n",
    "class TextClassificationModel(nn.Module):  # inherits from nn.Module\n",
    "    def __init__(self,vocab_size, embed_dim, num_class): \n",
    "        # super() access parent class (nn.Module) methods \n",
    "        super().__init__() # initialize the base(constructor) nn.Module \n",
    "        # voccab_size -> # of unique tokens \n",
    "        # embed_dim -> dim of word embedding (each word is represented by embed_dim dimension vector)\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size,embed_dim,sparse = True)\n",
    "        # fully connected layer \n",
    "        ## a simple linear layer that projects the final embedding to class logits.\n",
    "        self.fc = nn.Linear(embed_dim,num_class)\n",
    "        # call function init_weights to initiate model weight \n",
    "        self.init_weights() \n",
    "\n",
    "    # probably not necessary, but I will have one just for good practice \n",
    "    def init_weights(self): \n",
    "        initrange = 0.5 \n",
    "        self.embedding.weight.data.uniform_(-initrange,initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange,initrange)\n",
    "        self.fc.bias.data.zero_() # initialize biases to zero \n",
    "\n",
    "    def forward(self,text,offset): \n",
    "        # embedded is a tensor shape of [batch size, embed_dim], which is pooled embedding of a doc \n",
    "        embedded = self.embedding(text,offset) # text is 1D tensor \n",
    "        return self.fc(embedded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5202d3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model, loss, optimizer, and scheduler \n",
    "\n",
    "num_class = 4 # business, sci, sports, world \n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 64 \n",
    "\n",
    "# init model \n",
    "model = TextClassificationModel(vocab_size=vocab_size,embed_dim=embed_dim,num_class=num_class)\n",
    "\n",
    "# cross entropy loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1)\n",
    "# scheduler \n",
    "## step size -> period of learning rate decay \n",
    "## gamma -> multiplicative factor of learning rate decay \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size = 1,gamma = 0.9) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e1d3841",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'aten::_foreach_norm.Scalar' with arguments from the 'SparseCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_foreach_norm.Scalar' is only available for these backends: [CPU, MPS, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterCPU.cpp:31034 [kernel]\nMPS: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:39 [backend fallback]\nBackendSelect: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:491 [backend fallback]\nFunctionalize: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:280 [backend fallback]\nNamed: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:63 [backend fallback]\nAutogradOther: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradCPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradCUDA: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradHIP: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradXLA: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradMPS: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradIPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradXPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradHPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradVE: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradLazy: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradMeta: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradMTIA: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradPrivateUse1: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradPrivateUse2: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradPrivateUse3: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradNestedTensor: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nTracer: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16726 [kernel]\nAutocastCPU: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:487 [backend fallback]\nAutocastCUDA: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:354 [backend fallback]\nFuncTorchBatched: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:815 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:1073 [backend fallback]\nVmapMode: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:210 [backend fallback]\nPythonTLSSnapshot: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:152 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:487 [backend fallback]\nPythonDispatcher: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m total_loss\u001b[38;5;241m/\u001b[39mtotal_count, total_acc\u001b[38;5;241m/\u001b[39mtotal_count\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m15\u001b[39m): \n\u001b[0;32m---> 34\u001b[0m     loss,acc \u001b[38;5;241m=\u001b[39m train(train_dataloader) \n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# scheduler step \u001b[39;00m\n\u001b[1;32m     36\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep() \n",
      "Cell \u001b[0;32mIn[19], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataloder)\u001b[0m\n\u001b[1;32m     18\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward() \n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# gradient clipping (prevents explosion)\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(),max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# apply gradient update \u001b[39;00m\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep() \n",
      "File \u001b[0;32m/opt/anaconda3/envs/interviews/lib/python3.11/site-packages/torch/nn/utils/clip_grad.py:55\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ((device, _), [grads]) \u001b[38;5;129;01min\u001b[39;00m grouped_grads\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (foreach \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m foreach) \u001b[38;5;129;01mand\u001b[39;00m _has_foreach_support(grads, device\u001b[38;5;241m=\u001b[39mdevice):\n\u001b[0;32m---> 55\u001b[0m         norms\u001b[38;5;241m.\u001b[39mextend(torch\u001b[38;5;241m.\u001b[39m_foreach_norm(grads, norm_type))\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m foreach:\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforeach=True was passed, but can\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mt use the foreach API on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tensors\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'aten::_foreach_norm.Scalar' with arguments from the 'SparseCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_foreach_norm.Scalar' is only available for these backends: [CPU, MPS, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterCPU.cpp:31034 [kernel]\nMPS: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:39 [backend fallback]\nBackendSelect: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:491 [backend fallback]\nFunctionalize: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:280 [backend fallback]\nNamed: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:63 [backend fallback]\nAutogradOther: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradCPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradCUDA: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradHIP: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradXLA: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradMPS: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradIPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradXPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradHPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradVE: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradLazy: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradMeta: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradMTIA: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradPrivateUse1: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradPrivateUse2: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradPrivateUse3: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradNestedTensor: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nTracer: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16726 [kernel]\nAutocastCPU: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:487 [backend fallback]\nAutocastCUDA: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:354 [backend fallback]\nFuncTorchBatched: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:815 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:1073 [backend fallback]\nVmapMode: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:210 [backend fallback]\nPythonTLSSnapshot: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:152 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:487 [backend fallback]\nPythonDispatcher: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "# Model Training \n",
    "def train(dataloder): # dataloader iterate over taining batch \n",
    "    # tells the model to operate in training mode \n",
    "    model.train()\n",
    "    # metrics \n",
    "    ## total_acc -> total # of correct prediction \n",
    "    ## total_loss -> total loss over batches \n",
    "    ## total_count -> total # of samples seen \n",
    "    total_acc, total_loss, total_count = 0,0,0\n",
    "\n",
    "    # dataloader contains label, text, offset \n",
    "    for label,text,offset in dataloder: \n",
    "        # clear old gradients \n",
    "        optimizer.zero_grad()\n",
    "        output = model(text,offset) # nn.Module calls model.__call__() which calls forward() \n",
    "        loss = criterion(output,label) # compute how different the predicted vs. actual probabilities are \n",
    "        # backpropagation \n",
    "        loss.backward() \n",
    "        # gradient clipping (prevents explosion)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm=0.5)\n",
    "        # apply gradient update \n",
    "        optimizer.step() \n",
    "\n",
    "        # accumulate metrics \n",
    "        total_loss += loss.item() # float value from loss tensor \n",
    "        # argmax(1) find predicted label compare to true label and sum over bool tensor \n",
    "        total_acc += (output.argmax(1)==label).sum().item() \n",
    "        total_count += label.size(0) \n",
    "\n",
    "    # return avg loss and accuracy \n",
    "    return total_loss/total_count, total_acc/total_count\n",
    "    \n",
    "for epoch in range(15): \n",
    "    loss,acc = train(train_dataloader) \n",
    "    # scheduler step \n",
    "    scheduler.step() \n",
    "    print(f\"Epoch {epoch+1}: Accuracy = {acc}, Loss = {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8e4d575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0b0047",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interviews",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
