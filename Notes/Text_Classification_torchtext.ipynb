{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e50aa8f9",
   "metadata": {},
   "source": [
    "# Learning Project: Document Classification with PyTorch and TorchText\n",
    "\n",
    "This project builds a text classification model using the AG_NEWS dataset with PyTorch and TorchText. The model predicts the category of a news article (World, Sports, Business, or Sci/Tech) from its raw text. It uses:\n",
    "- __Tokenization__ and __vocabulary building__ to convert raw text into numeric format\n",
    "- A __collate function__ with EmbeddingBag for efficient text representation without padding\n",
    "- A __simple feedforward neural network__ for classification\n",
    "- __Cross-entropy loss__ and __stochastic gradient descent (SGD)__ for training\n",
    "- A learning __rate scheduler__ to dynamically adjust training speed\n",
    "\n",
    "The project demonstrates the full NLP pipeline: from data preprocessing to model training, evaluation, and optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0790b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries \n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b5d9278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split \n",
    "train_iter, test_iter = AG_NEWS(split=('train','test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c376b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizaiton and Vocab building \n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "# build vocabulary (with generator function to avoid memory inefficiency)\n",
    "vocab = build_vocab_from_iterator(\n",
    "    (tokenizer(text) for _,text in train_iter), \n",
    "    specials=['<unk>']\n",
    ")\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c5b4e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate function \n",
    "## collate function essentially preprocess the batch on the fly \n",
    "def collate_batch(batch): \n",
    "    # label_list -> store the true class label as int\n",
    "    # text_list -> store tokenized & indexed text \n",
    "    # offset -> starting index of each sample \n",
    "    label_list, text_list, offset = [],[],[0]\n",
    "    for label,text in batch: \n",
    "        label_list.append(label - 1) # -1 for 0 indexed \n",
    "        processed_text = torch.tensor(vocab(tokenizer(text)), dtype=torch.int64) # tokenized & numericalized\n",
    "        text_list.append(processed_text)\n",
    "        offset.append(processed_text.size(0)) # basically length of tensor .size is a tensor method to get dim\n",
    "    # convert label list to label tensor \n",
    "    label_tensor = torch.tensor(label_list,dtype=torch.int64)\n",
    "    # convert text list to text tensor \n",
    "    text_tensor = torch.cat(text_list)\n",
    "    # cumulative offsets: start index of each sample \n",
    "    offset_tensor = torch.tensor(offset[:-1]).cumsum(dim=0) # last length no needed; dim = 0 row wised \n",
    "\n",
    "    return label_tensor,text_tensor,offset_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbd30200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader \n",
    "from torch.utils.data.dataset import random_split\n",
    "# 95% for training and 5% for validation \n",
    "t_len = len(list(AG_NEWS(split='train')))\n",
    "num_train = int(t_len * 0.95)\n",
    "num_valid = t_len - num_train\n",
    "# randomly split training set into training and validation \n",
    "train_set, valid_set = random_split(list(AG_NEWS(split='train')), [num_train,num_valid])\n",
    "# wrap training set into train_dataloader \n",
    "train_dataloader = DataLoader(train_set,batch_size=8,shuffle=True,collate_fn=collate_batch)\n",
    "# wrap validation set into valid_dataloader \n",
    "valid_dataloader = DataLoader(valid_set,batch_size=8,collate_fn=collate_batch) # no shuffle, deterministic \n",
    "# wrap test set into test_dataloader \n",
    "test_dataloader = DataLoader(list(AG_NEWS(split='test')), batch_size=8,collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca159ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model \n",
    "class TextClassificationModel(nn.Module):  # inherits from nn.Module\n",
    "    def __init__(self,vocab_size, embed_dim, num_class): \n",
    "        # super() access parent class (nn.Module) methods \n",
    "        super().__init__() # initialize the base(constructor) nn.Module \n",
    "        # voccab_size -> # of unique tokens \n",
    "        # embed_dim -> dim of word embedding (each word is represented by embed_dim dimension vector)\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size,embed_dim,sparse = False)\n",
    "        # fully connected layer \n",
    "        ## a simple linear layer that projects the final embedding to class logits.\n",
    "        self.fc = nn.Linear(embed_dim,num_class)\n",
    "        # call function init_weights to initiate model weight \n",
    "        self.init_weights() \n",
    "\n",
    "    # probably not necessary, but I will have one just for good practice \n",
    "    def init_weights(self): \n",
    "        initrange = 0.5 \n",
    "        self.embedding.weight.data.uniform_(-initrange,initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange,initrange)\n",
    "        self.fc.bias.data.zero_() # initialize biases to zero \n",
    "\n",
    "    def forward(self,text,offset): \n",
    "        # embedded is a tensor shape of [batch size, embed_dim], which is pooled embedding of a doc \n",
    "        embedded = self.embedding(text,offset) # text is 1D tensor \n",
    "        return self.fc(embedded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5202d3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model, loss, optimizer, and scheduler \n",
    "\n",
    "num_class = 4 # business, sci, sports, world \n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 64 \n",
    "\n",
    "# init model \n",
    "model = TextClassificationModel(vocab_size=vocab_size,embed_dim=embed_dim,num_class=num_class)\n",
    "\n",
    "# cross entropy loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1)\n",
    "# scheduler \n",
    "## step size -> period of learning rate decay \n",
    "## gamma -> multiplicative factor of learning rate decay \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size = 1,gamma = 0.9) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e1d3841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Accuracy = 0.8535438596491228, Loss = 0.0523283961474969\n",
      "Epoch 2: Accuracy = 0.9116491228070176, Loss = 0.033529493554022974\n",
      "Epoch 3: Accuracy = 0.9254561403508772, Loss = 0.02869510322029594\n",
      "Epoch 4: Accuracy = 0.9331754385964912, Loss = 0.02553864622307485\n",
      "Epoch 5: Accuracy = 0.9401754385964912, Loss = 0.023260910315879235\n",
      "Epoch 6: Accuracy = 0.9457280701754386, Loss = 0.021346841640920804\n",
      "Epoch 7: Accuracy = 0.9504122807017544, Loss = 0.01976001121050279\n",
      "Epoch 8: Accuracy = 0.9547543859649122, Loss = 0.018311474884890152\n",
      "Epoch 9: Accuracy = 0.9576578947368422, Loss = 0.017151077284391615\n",
      "Epoch 10: Accuracy = 0.9608333333333333, Loss = 0.016152021202176146\n",
      "Epoch 11: Accuracy = 0.9638947368421052, Loss = 0.015269376229713392\n",
      "Epoch 12: Accuracy = 0.9659824561403508, Loss = 0.014560792648488738\n",
      "Epoch 13: Accuracy = 0.9681052631578947, Loss = 0.013870331654276718\n",
      "Epoch 14: Accuracy = 0.9694649122807018, Loss = 0.013334292842408743\n",
      "Epoch 15: Accuracy = 0.9715350877192982, Loss = 0.01279999476986808\n"
     ]
    }
   ],
   "source": [
    "# Model Training \n",
    "def train(dataloder): # dataloader iterate over taining batch \n",
    "    # tells the model to operate in training mode \n",
    "    model.train()\n",
    "    # metrics \n",
    "    ## total_acc -> total # of correct prediction \n",
    "    ## total_loss -> total loss over batches \n",
    "    ## total_count -> total # of samples seen \n",
    "    total_acc, total_loss, total_count = 0,0,0\n",
    "\n",
    "    # dataloader contains label, text, offset \n",
    "    for labels,text,offsets in dataloder: \n",
    "        # clear old gradients \n",
    "        optimizer.zero_grad()\n",
    "        output = model(text,offsets) # nn.Module calls model.__call__() which calls forward() \n",
    "        loss = criterion(output,labels) # compute how different the predicted vs. actual probabilities are \n",
    "        # backpropagation \n",
    "        loss.backward() \n",
    "        # gradient clipping (prevents explosion)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm=0.5)\n",
    "        # apply gradient update \n",
    "        optimizer.step() \n",
    "\n",
    "        # accumulate metrics \n",
    "        total_loss += loss.item() # float value from loss tensor \n",
    "        # argmax(1) find predicted label compare to true label and sum over bool tensor \n",
    "        total_acc += (output.argmax(1)==labels).sum().item() \n",
    "        total_count += labels.size(0) \n",
    "\n",
    "    # return avg loss and accuracy \n",
    "    return total_loss/total_count, total_acc/total_count\n",
    "    \n",
    "for epoch in range(15): \n",
    "    loss,acc = train(train_dataloader) \n",
    "    # scheduler step \n",
    "    scheduler.step() \n",
    "    print(f\"Epoch {epoch+1}: Accuracy = {acc}, Loss = {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f88af6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.9118333333333334\n",
      "Test accuracy: 0.9039473684210526\n"
     ]
    }
   ],
   "source": [
    "# evaluation \n",
    "def evaluate(dataloader): \n",
    "    # put model in evaluation mode \n",
    "    model.eval() \n",
    "    total_acc, total_count = 0,0 \n",
    "    # disable gradient computing or storing \n",
    "    with torch.no_grad(): \n",
    "        for labels,text,offsets in dataloader: \n",
    "            output = model(text,offsets)\n",
    "            total_acc += (output.argmax(1)==labels).sum().item()\n",
    "            total_count += labels.size(0) \n",
    "    return total_acc/total_count # accuracy \n",
    "\n",
    "print(f\"Validation accuracy: {evaluate(valid_dataloader)}\")\n",
    "print(f\"Test accuracy: {evaluate(test_dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98057a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, model, vocab, tokenizer):\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        # Tokenize and convert to vocab indices\n",
    "        tokens = torch.tensor(vocab(tokenizer(text)), dtype=torch.int64)\n",
    "        # Offsets tensor (starting index for the sentence in the batch)\n",
    "        offsets = torch.tensor([0])\n",
    "        # Get the model output\n",
    "        output = model(tokens, offsets)\n",
    "        # Get the predicted class index\n",
    "        predicted_label = output.argmax(1).item()\n",
    "        return predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04c12329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted category: Sports\n"
     ]
    }
   ],
   "source": [
    "test = 'I think sports like football can be dangerous.' \n",
    "label_index = predict(test,model,vocab,tokenizer)\n",
    "label_map = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
    "print(f\"Predicted category: {label_map[label_index]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interviews",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
