{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c55ebc34",
   "metadata": {},
   "source": [
    "# __Table of Contents__\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"#Introduction\">Introduction</a></li>\n",
    "    <li><a href=\"#Types-of-Tokenizer\">Types of Tokenizer</a></li>\n",
    "        <ol>\n",
    "            <li><a href=\"#Word-based-tokenizer\">Word-based tokenizer</a></li>\n",
    "            <li><a href=\"#Character-based-tokenizer\">Character-based tokenizer</a></li>\n",
    "            <li><a href=\"#Subword-based-tokenizer\">Subword-based tokenizer</a></li>\n",
    "                <ol>\n",
    "                    <li><a href=\"#WordPiece\">WordPiece</a></li>\n",
    "                    <li><a href=\"#Unigram-and-SentencePiece\">Unigram and SentencePiece</a></li>\n",
    "                </ol>\n",
    "        </ol>\n",
    "    <li>\n",
    "        <a href=\"#Tokenization-with-PyTorch\">Tokenization with PyTorch</a>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Token-indices\">Token indices</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Out-of-vocabulary-(OOV)\">Out-of-vocabulary (OOV)</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><a href=\"#Exercise:-Comparative-text-tokenization-and-performance-analysis\">Exercise: Comparative text tokenization and performance analysis</a></li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4b7574",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "**Tokenization** is the process of breaking down a piece of text, such as a sentence or paragraph, into smaller units called **tokens**.\n",
    "\n",
    "## Libraries \n",
    "\n",
    "1. **NLTK (Natural Language Toolkit)**\n",
    "    - A classic, general-purpose NLP toolkit used for teaching and prototyping.\n",
    "    - Provides rule-based word tokenization and other basic preprocessing tools.\n",
    "    - Not designed for deep learning models but great for traditional NLP tasks.\n",
    "2.  **spaCy**\n",
    "    - An industrial-strength NLP pipeline designed for speed and efficiency.\n",
    "    - Offers fast, rule-based tokenization along with POS tagging, named entity recognition, and dependency parsing.\n",
    "    - Commonly used in production environments.\n",
    "3. **BertTokenizer (Hugging Face Transformers)**\n",
    "    - A subword-based tokenizer using the **WordPiece** algorithm.\n",
    "    - Specifically designed to tokenize text for input into **BERT** models.\n",
    "    - Ensures compatibility with pre-trained transformer architectures.\n",
    "4. **XLNetTokenizer (Hugging Face Transformers)**\n",
    "    - Implements **Unigram** or **SentencePiece**-based tokenization.\n",
    "    - Tailored for XLNet's architecture, which uses permutation-based language modeling.\n",
    "    - Tokens often include underscores to indicate word boundaries.\n",
    "5. **torchtext**\n",
    "    - Part of the PyTorch ecosystem, focusing on NLP data processing.\n",
    "    - Provides tokenization, vocabulary construction, and batching.\n",
    "    - Compatible with user-defined tokenizers and custom data pipelines.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d74467",
   "metadata": {},
   "source": [
    "## Types of Tokenizer \n",
    "\n",
    "### Word-Based Tokenization\n",
    "- In this method, the text is split into individual words.\n",
    "- Each word is treated as a single token.\n",
    "- Example:\n",
    "  - Input: `\"Tokenization is important.\"`\n",
    "  - Output: `[\"Tokenization\", \"is\", \"important\", \".\"]`\n",
    "\n",
    "**Advantages:**\n",
    "- Preserves the meaning of entire words.\n",
    "- Simple and intuitive.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Results in a very large vocabulary.\n",
    "- Treats similar words (e.g., \"run\", \"running\") as completely different tokens.\n",
    "- Cannot handle out-of-vocabulary (OOV) words effectively.\n",
    "\n",
    "---\n",
    "\n",
    "### Character-Based Tokenization\n",
    "- This approach splits text into individual characters.\n",
    "- Each character is treated as a token.\n",
    "- Example:\n",
    "  - Input: `\"NLP\"`\n",
    "  - Output: `[\"N\", \"L\", \"P\"]`\n",
    "\n",
    "**Advantages:**\n",
    "- Very small vocabulary size.\n",
    "- Can handle any word, including unseen ones.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Loses semantic meaning of words.\n",
    "- Requires longer sequences and more computation.\n",
    "- Harder for the model to learn language structure.\n",
    "\n",
    "---\n",
    "\n",
    "### Subword-Based Tokenization\n",
    "- A hybrid approach that breaks text into smaller word components (subwords).\n",
    "- Frequently occurring words may be kept whole, while rare or unknown words are split into subword units.\n",
    "- Used in modern transformer models like BERT and XLNet.\n",
    "\n",
    "**Examples of subword algorithms:**\n",
    "- WordPiece (used in BERT)\n",
    "- Unigram (used in XLNet)\n",
    "- SentencePiece (used in T5 and others)\n",
    "\n",
    "**Example:**\n",
    "- Input: `\"tokenization\"`\n",
    "- Output (WordPiece): `[\"token\", \"##ization\"]`\n",
    "\n",
    "**Advantages:**\n",
    "- Balances vocabulary size and expressiveness.\n",
    "- Can handle OOV words by composing them from subwords.\n",
    "- More efficient than word-based models in terms of vocabulary.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Slightly more complex to implement and interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528cd3f2",
   "metadata": {},
   "source": [
    "### Word-based tokenizer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b698770e",
   "metadata": {},
   "source": [
    "### Subword-based tokenizer\n",
    "#### WordPiece (Used in BERT)\n",
    "\n",
    "- **Core idea**: WordPiece builds its vocabulary by starting with all characters in the training data and **progressively merging subword units** to maximize the likelihood of the training corpus.\n",
    "- Unlike Byte Pair Encoding (BPE), which selects the most frequent pair, **WordPiece chooses the merge that improves data likelihood the most**.\n",
    "- During tokenization, WordPiece applies learned merge rules greedily to segment a word into subword units.\n",
    "- **Implementation**: Used by `BertTokenizer` in Hugging Face.\n",
    "- **Special markers**: Uses `##` to indicate that a token is a continuation of a word.\n",
    "  \n",
    "**Example**:  \n",
    "Input: `\"tokenization\"`  \n",
    "Tokens: `[\"token\", \"##ization\"]`\n",
    "\n",
    "\n",
    "\n",
    "#### Unigram Language Model (Used in XLNet, SentencePiece)\n",
    "\n",
    "- **Core idea**: The Unigram model starts with a very large list of possible subword candidates and then **prunes the vocabulary** by removing those that contribute the least to the likelihood of the data.\n",
    "- It is **probabilistic**, meaning it allows for multiple segmentations and selects the most likely one based on the model.\n",
    "- Unlike WordPiece, it does not build vocabulary through merging, but through iterative elimination.\n",
    "- **Flexible and powerful** for multilingual or noisy text.\n",
    "\n",
    "\n",
    "\n",
    "#### SentencePiece (Tokenizer Framework)\n",
    "\n",
    "- **Core idea**: SentencePiece is not a tokenization algorithm itself but a **tokenizer framework** that supports both Unigram and BPE.\n",
    "- It treats input as **raw text without any whitespace pre-tokenization**, making it **language-agnostic**.\n",
    "- SentencePiece also ensures **consistency and reproducibility**, assigning unique IDs to subwords so that the same text always results in the same tokens and indices.\n",
    "- Can be trained with either Unigram or BPE strategies.\n",
    "\n",
    "**Example**:  \n",
    "Input: `\"Tokenization is powerful\"`  \n",
    "Tokens: `[\"▁Token\", \"ization\", \"▁is\", \"▁power\", \"ful\"]`  \n",
    "(Note: `▁` marks word boundaries)\n",
    "\n",
    "\n",
    "\n",
    "#### Integration: SentencePiece + Unigram\n",
    "\n",
    "- SentencePiece is often used to **implement Unigram tokenization**.\n",
    "- SentencePiece handles the **training, segmentation, and ID assignment**, while **Unigram** guides the vocabulary pruning process to optimize token efficiency.\n",
    "- This combination is widely used in multilingual models such as **mT5**, **ALBERT**, and **XLNet**.\n",
    "\n",
    "\n",
    "\n",
    "#### Summary Comparison\n",
    "\n",
    "| Feature                          | WordPiece            | Unigram                    | SentencePiece              |\n",
    "|----------------------------------|-----------------------|----------------------------|----------------------------|\n",
    "| Strategy                         | Merge-based           | Prune-based (likelihood)   | Tokenizer framework        |\n",
    "| Probabilistic?                   | No                    | Yes                        | Supports probabilistic (Unigram) |\n",
    "| Pre-tokenization (e.g. by spaces) | Required              | Not required               | Not required               |\n",
    "| Special Markers                  | `##` for subword      | `▁` (via SentencePiece)    | `▁` (underscores)          |\n",
    "| Vocabulary Learning              | Greedy merges         | Vocabulary reduction       | Supports both BPE & Unigram |\n",
    "| Common Usage                     | BERT, RoBERTa         | XLNet, mT5, ALBERT         | T5, mT5, ALBERT, XLNet     |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c6c0cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89b6289b7bec41aca818e51044a81aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96191742e2a6445bbe45b2e2194e7b6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "788350ee90274a288c4673654a598a11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46edb542b2384a6b90dd6bc14234f02e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['ibm', 'taught', 'me', 'token', '##ization', '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer.tokenize(\"IBM taught me tokenization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616f1058",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interviews",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
